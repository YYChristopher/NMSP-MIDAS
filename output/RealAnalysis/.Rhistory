beta <- matrix(0, nrow = NS, ncol = ng * sg[1])
betan <- matrix(0, nrow = NS, ncol = ng)
kappabar <- rep((1 + 1 / ng), NS)
u <- kappabar
aa <- kappabar * (ng^u) # stands for c in paper
bb <- rep(1, NS) # stands for d in paper
pi0 <- rbeta(NS, shape1 = aa, shape2 = bb, ncp = 0)
pi1 <- matrix(runif(NS * ng), nrow = NS, ncol = ng)
Z <- matrix(0, nrow = NS, ncol = ng)
x_NT=c()
for(i in 1:NT){
x_NT=rbind(x_NT,x[,i,])
}
y_NT=c()
for(i in 1:NT){
y_NT=c(y_NT,y[i, ])
}
d_NT=c()
for(i in 1:NT){
d_NT=rbind(d_NT,d[,i,])
}
fc=array(NA,dim=c(N,NT,Nbvar*eg+egd+1))
fc_NT=array(NA,dim=c(N*NT,Nbvar*eg+egd+1))
# fc=array(NA,dim=c(N,NT,Nbvar*eg+1))
# fc_NT=array(NA,dim=c(N*NT,Nbvar*eg+1))
if(Nbvar!=0){
fc_NT=cbind(x_NT[,1:((Nbvar+1)*eg)],rep(1,N*NT))
# fc_NT=cbind(x_NT[,1:(Nbvar*eg)],rep(1,N*NT))
for(i in 1:NT){
fc[,i,]=fc_NT[(1+(i-1)*N):(i*N),]
}
}else{
fc_NT[,1]=rep(1,N*NT)
for(i in 1:NT){
fc[,i,]=fc_NT[(1+(i-1)*N):(i*N)]
}
}
# tuning parameters for MH algorithm
c_lop = 15
c_zeta = 12
c_phi = 3
cal = rep(0, N)
## initial value of parameters
alpha_cpp = runif(NS * (eg * (Nbvar + 1) + 1), -1, 1)
# alpha_cpp = runif(NS * (eg * Nbvar + 1), -1, 1)
lop_cpp = runif(NS - 1, -1, 1)
zeta = array(NA, dim = c(NS, NS))
zeta[ ,NS] = 0
zeta[ ,1:(NS - 1)] = runif(NS * (NS - 1), -1, 1)
zeta_cpp = as.vector(t(zeta))
phi_cpp = runif(NH, -1, 1)
betan_cpp = as.vector(betan)
lambda2_cpp = as.vector(lambda2)
tau2_cpp = as.vector(tau2)
pi0_cpp = as.vector(pi0)
pi1_cpp = as.vector(pi1)
Z_cpp = as.vector(Z)
groups = as.vector(t(groups))
sigma_cpp = runif(NS, 0, 1)
s_cpp = sample(0:(NS - 1), N * NT, replace = TRUE)
# s_cpp = as.vector(real_state) - 1
y_cpp = y_NT
x_cpp = as.vector(x_NT)
fc_cpp = as.vector(fc_NT)
d_cpp = as.vector(d_NT)
priorbeta = c(0, 0, 0)
ele = c(40, 40)
#iterations
iter = 1   ## number of replication
rep = 1    ## current replication
It = vector('list',3)
names(It) = c('nsave','nburn','nthin')
It$nsave = 10000 # total sample
It$nburn = 10000 # burn sample
It$nthin = 1
ntotal = It$nsave + It$nburn
ndraw = It$nsave / It$nthin
# Update and Iter times for Gibbs Sampling
nsave <- It$nsave
nburn <- It$nburn
nthin <- It$nthin
ntot <- nsave + nburn
ndraw <- nsave / nthin
seed1 = as.numeric(Sys.time()) + sample(1:10000, 1, replace = FALSE)
# seed1 = 1730975314
set.seed(seed1)
# result = mcmc(y_cpp, x_cpp, fc_cpp, d_cpp, alpha_cpp, sigma_cpp, tau_cpp, zeta_cpp, phi_cpp, s_cpp, N, NT, NS, NF, ND, c_tau, c_zeta, c_phi, iter, rep, ntot, ndraw)
start.time = Sys.time()
result = mcmc_hmm(y_cpp, x_cpp, fc_cpp, alpha_cpp,
sigma_cpp, betan_cpp, lambda2_cpp, tau2_cpp, pi0_cpp,
pi1_cpp, Z_cpp, sg, groups, s_cpp, priorbeta,
N, NT, NS, NF, iter, rep, ntot, ndraw, ng, eg, ele)
end.time = Sys.time()
# variables <- ls()
# sizes <- sapply(variables, function(x) object.size(get(x)))
# df <- data.frame(Variable = variables, Size = sizes / (1024 * 1024)) # 杞崲涓? MB
# print(sum(df$Size))
# df <- df[order(df$Size), ]
# View(df)
## collect the estimates of MCMC iteration
sigma_result = array(result$sigma_str, dim = c(NS, ndraw))
alpha_result = array(0, dim = c(ndraw, NS, NF + 1))
for(i in 1:NS){
for(j in 1:(NF + 1)){
alpha_result[ ,i ,j] = result$alpha_str[(0:(ndraw - 1)) * NS * (NF + 1) +(i - 1) * (NF + 1) + j]
}
}
p_result = array(0, dim = c(ndraw, NS, NS))
for(i in 1:NS){
for(j in 1:NS){
p_result[ ,i ,j] = result$p_str[(0:(ndraw - 1)) * NS * NS + (i - 1) * NS + j]
}
}
state_result = array(0, dim = c(ndraw, N, NT))
for(i in 1:NT){
for(j in 1:N){
state_result[ ,j ,i] = result$s_str[(0:(ndraw - 1)) * NT * N + (i - 1) * N + j]
}
}
pi1_result = array(0, dim = c(ndraw, NS, ng))
for(i in 1:NS){
for(j in 1:ng){
pi1_result[ ,i ,j] = result$pi1_str[(0:(ndraw - 1)) * NS * ng + (i - 1) * ng + j]
}
}
## posterior mean
sigma.estimation = apply(sigma_result, 1, mean)
alpha.estimation = apply(alpha_result, c(2:3), mean)
intercept.estimation = alpha.estimation[ , ncol(alpha.estimation)]
pi1.estimation = apply(pi1_result, c(2:3), mean)
sigma.sd = apply(sigma_result, 1, sd)
alpha.sd = apply(alpha_result, c(2:3), sd)
intercept.sd = alpha.sd[ , ncol(alpha.estimation)]
# sigma.estimation2 = apply(sigma_result, 1, median)
# alpha.estimation2 = apply(alpha_result, c(2:3), median)
# intercept.estimation2 = alpha.estimation2[ , ncol(alpha.estimation)]
sigma.estimation
intercept.estimation
alpha.estimation
sigma.sd
alpha.sd
state.res = array(0, dim=c(N, NT))
temp.mat = matrix(0, nrow = NT, ncol = NS)
for(i in 1:N){
for(j in 1:NT){
temp = rep(0,NS)
for(k in 1:NS){
temp[k] = sum(state_result[ ,i,j] == (k - 1))
}
temp.mat[j, ] = temp
state.res[i, j] = which.max(temp)
}
}
state.estimation = c(state.res)
state.estimation
sig_x_NS = matrix(0, nrow = NS, ncol = Nbvar * eg + egd)
mu_x_NS = matrix(0, nrow = NS, ncol = Nbvar * eg + egd)
# sig_x_NS = matrix(0, nrow = NS, ncol = Nbvar * eg)
# mu_x_NS = matrix(0, nrow = NS, ncol = Nbvar * eg)
beta.estimation = alpha.estimation[ ,-ncol(alpha.estimation)]
beta.real.estimation = matrix(0, nrow = NS, ncol = Nbvar + 1)
beta.real.estimation2 = matrix(0, nrow = NS, ncol = Nbvar + 1)
# beta.real.estimation = matrix(0, nrow = NS, ncol = Nbvar)
# beta.real.estimation2 = matrix(0, nrow = NS, ncol = Nbvar)
# alpha.no.int = alpha.estimation[ ,1:(Nbvar * eg)]
alpha.no.int = alpha.estimation[ ,1:(Nbvar * eg + egd)]
for (s in 1:NS) {
# stdres = standardize_me2(QX[state == s, ])
sig_x_NS[s, ] = standardize_me2(QX[state.estimation == s, ])$sig
mu_x_NS[s, ] = standardize_me2(QX[state.estimation == s, ])$mu
intercept.estimation[s] = intercept.estimation[s] + mean(Y)
# intercept.estimation[s] = intercept.estimation[s] + mean(Y) - sum(alpha.no.int[s, ] * mu_x / sig_x)
# intercept.estimation[s] = intercept.estimation[s] + mean(Y) - sum(alpha.no.int[s, ] * mu_x_NS[s, ] / sig_x_NS[s, ])
for (nb in 1:Nbvar) {
beta.real.estimation[s, nb] = sum((alpha.estimation[s, ((nb - 1)*eg + 1):(nb*eg)] / sig_x_NS[s, ((nb - 1)*eg + 1):(nb*eg)]) %*% Q)
beta.real.estimation2[s, nb] = sum((alpha.estimation[s, ((nb - 1)*eg + 1):(nb*eg)] / sig_x[((nb - 1)*eg + 1):(nb*eg)]) %*% Q)
}
beta.real.estimation[s, Nbvar + 1] = sum((alpha.estimation[s, (Nbvar*eg + 1):(Nbvar*eg + egd)] / sig_x_NS[s, (Nbvar*eg + 1):(Nbvar*eg + egd)]) %*% Qd)
beta.real.estimation2[s, Nbvar + 1] = sum((alpha.estimation[s, (Nbvar*eg + 1):(Nbvar*eg + egd)] / sig_x[(Nbvar*eg + 1):(Nbvar*eg + egd)]) %*% Qd)
}
intercept.estimation
beta.real.estimation
beta.real.estimation2
which(abs(beta.real.estimation[1,])>0.1)
which(abs(beta.real.estimation[2,])>0.1)
apply(p_result, c(2, 3), mean)
# Give Credibal Interval
alpha.CI <- apply(alpha_result, c(2, 3), function(x) quantile(x, probs = c(0.025, 0.975)))
intercept.CI <- alpha.CI[ , ,dim(alpha.CI)[3]] + mean(Y)
sigma.CI <- apply(sigma_result, c(1), function(x) quantile(x, probs = c(0.025, 0.975)))
beta.real.CI = array(0, dim = c(2, NS, Nbvar))
for (ci in 1:2) {
for (s in 1:NS) {
sig_x_NS[s, ] = standardize_me2(QX[state.estimation == s, ])$sig
mu_x_NS[s, ] = standardize_me2(QX[state.estimation == s, ])$mu
for (nb in 1:Nbvar) {
beta.real.CI[ci, s, nb] = sum((alpha.CI[ci, s, ((nb - 1)*eg + 1):(nb*eg)] / sig_x_NS[s, ((nb - 1)*eg + 1):(nb*eg)]) %*% Q)
}
}
}
beta_result = array(0, dim = c(ndraw, NS, Nbvar + 1))
for (i in 1:ndraw) {
for (s in 1:NS) {
for (nb in 1:Nbvar) {
beta_result[i, s, nb] = sum((alpha_result[i, s, ((nb - 1)*eg + 1):(nb*eg)] / sig_x_NS[s, ((nb - 1)*eg + 1):(nb*eg)]) %*% Q)
}
beta_result[i, s, Nbvar + 1] = sum((alpha_result[i, s, (Nbvar*eg + 1):(Nbvar*eg + egd)] / sig_x_NS[s, (Nbvar*eg + 1):(Nbvar*eg + egd)]) %*% Qd)
}
}
beta.sd = apply(beta_result, c(2:3), sd)
################################## Plot ########################################
NBER = read.csv('RealData/USREC.csv', header = TRUE)
NBER_noindex <- NBER['USREC']
dates_nber_all <- seq(as.Date("1854-12-01"), by = "month", length.out = dim(NBER)[1])
NBER_xts <- xts(NBER_noindex, order.by = dates_nber_all)
dates_nber.quarter <- seq(as.Date("1960-01-01"), by = "quarter", length.out = NT_train)
dates_nber.month <- seq(as.Date("1960-01-01"), by = "month", length.out = NT_train * m_mon)
NBER_used <- NBER_xts['1960-01-01/2010-12-01']
df.nber <- data.frame(
Date = dates_nber.month,
Recession = c(as.numeric(NBER_used))
)
recession_periods <- subset(df.nber, NBER == 1)
df.nber$Date <- as.Date(df.nber$Date)
# 鍒涘缓鍖呭惈琛伴€€鏈熺殑寮€濮嬪拰缁撴潫鏃堕棿鐨勬暟鎹
recession_periods <- df.nber %>%
mutate(Recession_Change = Recession != lag(Recession, default = 0)) %>%
mutate(Period_ID = cumsum(Recession_Change)) %>%
group_by(Period_ID) %>%
filter(Recession == 1) %>%
summarize(Start = min(Date), End = max(Date))
# 鍖哄垎棰滆壊
df.gdp.growth.color <- data.frame(
Date = dates_nber.quarter,
GDP.growth = c(Y),
# state = as.character(real_state)
state = as.character(state.estimation)
)
plot.gdp.growth.color <- ggplot() +
geom_rect(data = recession_periods, aes(xmin = Start, xmax = End, ymin = -2, ymax = 6),
fill = "grey", alpha = 0.5) +
geom_point(data = df.gdp.growth.color, aes(x = Date, y = GDP.growth, color = state)) +
scale_color_manual(values = c("1" = "red", "2" = "blue")) +
labs(y = TeX("$y_t"), x = "Date") +
theme_minimal() +
theme(
axis.title.x = element_text(size = 16),  # x 杞村悕绉板瓧浣撳ぇ灏?
axis.title.y = element_text(size = 16),  # y 杞村悕绉板瓧浣撳ぇ灏?
axis.text = element_text(size = 12),     # 鍧愭爣杞村埢搴﹀瓧浣撳ぇ灏?
plot.title = element_text(size = 18, face = "bold"),  # 鏍囬瀛椾綋澶у皬
legend.text = element_text(size = 14),   # 鍥句緥瀛椾綋澶у皬
legend.title = element_text(size = 16)   # 鍥句緥鏍囬瀛椾綋澶у皬
) +
scale_x_date(date_breaks = "4 year", date_labels = "%Y") +  # 姣忓勾鏄剧ず涓€涓棩鏈熸爣绛?
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # 鏃嬭浆x杞寸殑鏂囨湰
theme(axis.text.y = element_text())  # 淇濇寔y杞寸殑鏂囨湰
show(plot.gdp.growth.color)
################################## Forecast ########################################
######### Prepare Test Data ############
gdp <- GDP_xts['1959-03-01/2023-12-01']
gdp.test <- GDP_xts['2011-03-01/2023-12-01']
len.test = as.numeric(length(gdp.test))
gdp_rate_test = rep(0, len.test - 1)
for (i in 1:(len.test - 1)) {
gdp_rate_test[i] = 100 * log(as.numeric(gdp.test[i + 1]) / as.numeric(gdp.test[i]))
}
growth_dates_test = dates_test <- seq(as.Date("2011-03-01"), by = "quarter", length.out = len.test - 1)
gdp_growth_test = xts(gdp_rate_test, order.by = growth_dates_test)
gdp_growth_test_num = as.numeric(gdp_growth_test) # 鍖栦负numeric绫诲瀷鐢ㄤ簬鍒嗘瀽
gdp_growth.test <- gdp_growth_test["2011-01-01/2023-12-01"]
Y.test <- gdp_growth.test
fredmd.test <- fredmd[(NT_month.train - 12 + 1):(NT_month.train + NT_month.test), ]
# process NA
sum(is.na(fredmd.test))  # 387 NA need to be filled.
has_na <- colSums(is.na(fredmd.test)) > 0
na_columns <- names(fredmd.test)[has_na]
na_columns
na_counts <- colSums(is.na(fredmd.test[has_na]))
na_counts
nac1 <- as.numeric(fredmd.test$`S&P div yield`)
nac2 <- as.numeric(fredmd.test$`S&P PE ratio`)
nac3 <- as.numeric(fredmd.test$CP3Mx)
nac1_omit = na.omit(nac1)
nac2_omit = na.omit(nac2)
nac3_omit = na.omit(nac3)
nac1_mean = mean(nac1_omit)
nac1_sd = sd(nac1_omit)
nac2_mean = mean(nac2_omit)
nac2_sd = sd(nac2_omit)
nac3_mean = mean(nac3_omit)
nac3_sd = sd(nac3_omit)
nac1_count <- sum(is.na(fredmd.test$`S&P div yield`))  # 璁＄畻A鍒楃殑NA鏁伴噺
fill_values1 <- rnorm(nac1_count, mean = nac1_mean, sd = nac1_sd)
fredmd.test$`S&P div yield`[is.na(fredmd.test$`S&P div yield`)] <- fill_values1
nac2_count <- sum(is.na(fredmd.test$`S&P PE ratio`))  # 璁＄畻A鍒楃殑NA鏁伴噺
fill_values2 <- rnorm(nac2_count, mean = nac2_mean, sd = nac2_sd)
fredmd.test$`S&P PE ratio`[is.na(fredmd.test$`S&P PE ratio`)] <- fill_values2
nac3_count <- sum(is.na(fredmd.test$CP3Mx))  # 璁＄畻A鍒楃殑NA鏁伴噺
fill_values3 <- rnorm(nac3_count, mean = nac3_mean, sd = nac3_sd)
fredmd.test$CP3Mx[is.na(fredmd.test$CP3Mx)] <- fill_values3
fredmd.test$`S&P div yield` <- as.numeric(fredmd.test$`S&P div yield`)
fredmd.test$CP3Mx <- as.numeric(fredmd.test$CP3Mx)
# 涓綅鏁板～琛ユ柟娉?(鐢ㄤ簬缂哄け鍊兼暟閲忎笉澶х殑鎯呭舰)
for(i in 1:ncol(fredmd.test)) {
fredmd.test[ , i][is.na(fredmd.test[ , i])] <- median(fredmd.test[ , i], na.rm=TRUE)
}
fredmd.test <- as.data.frame(fredmd.test)
# 妫€鏌ヤ竴涓嬫湁鏃犵己澶卞€?
has_na <- colSums(is.na(fredmd.test)) > 0
na_columns <- names(fredmd.test)[has_na]
na_columns
na_counts <- colSums(is.na(fredmd.test[has_na]))
na_counts
# summary descriptive statistics
fredmd.test.summ <- sapply(fredmd.test, function(x) c(Mean = mean(x, na.rm = TRUE),
SD = sd(x, na.rm = TRUE),
Min = min(x, na.rm = TRUE),
Max = max(x, na.rm = TRUE)))
fredmd.test <- subset(fredmd.test, select = -c(VIXCLSx))
all.names <- c(names(fredmd.test))
fredmd.test <- subset(fredmd.test, select = -c(FEDFUNDS, CP3Mx, TB3MS, TB6MS, GS1, GS5, GS10, AAA, BAA))
fredmd.test[, "HWI"] <- fredmd.test[, "HWI"] / 100
fredmd.test[, c("CES0600000007", "AWHMAN")] <- fredmd.test[, c("CES0600000007", "AWHMAN")] / 10
ff3.test <- ff3[(NT_month.train - 12 + 1):(NT_month.train + NT_month.test), ]
ads.test.org <- ads_xts['2010-12-01/2023-12-01']
Xm.test.org <- as.data.frame(cbind(fredmd.test, ff3.test))
Xm.test <- as.data.frame(matrix(0, nrow = nrow(Xm.test.org), ncol = ncol(Xm.test.org)))
for (m in 1:ncol(Xm.test)) {
Xm.test[ ,m] <- (Xm.test.org[ ,m] - Xm.mean[m]) / Xm.std[m]
}
ads.test <- (ads.test.org - ads.mean) / ads.std
NT_gdp.test = as.numeric(nrow(gdp_growth.test))
NT_test <- NT_gdp.test
K_mon = Spc$Km # 娉ㄦ剰锛屽鏋淜_mon瑕佹槸涓€涓?>3鐨勫€硷紝闇€瑕佸啀鏀惧叆涓€浜涘墠椤圭殑鏁版嵁
m_mon = as.numeric(nrow(fredmd.test) %/% nrow(gdp_growth.test))
# Monthly matrix
xmls.test <- matrix(0, nrow = NT_gdp.test, ncol = 0)
xx.test <- vector("list", length = Nbvar) # monthly is Nbvar
for (ii in 1:Nbvar) {
xx.test[[ii]] <- Construct_DataMIDAS(g = gdp_growth.test, d = Xm.test[, ii], K = K_mon, m = m_mon)
xmls.test <- cbind(xmls.test, xx.test[[ii]])
}
xd2.test <- vector("list", length = 1)
m_day2.test = as.numeric(length(ads.test) %/% nrow(gdp_growth.test)) # 93
K_day2.test = m_day2.test
xd2.test[[1]] <- Construct_DataMIDAS(g = gdp_growth.test, d = ads.test, K = K_day2.test, m = m_day2.test)
xmls.test <- cbind(xmls.test, xd2.test[[1]][1:NT_gdp.test, ]) # dim(XX_all): 51 1545
X.test <- xmls.test[1:NT_gdp.test, ]
D_test <- D[(NT_train + 1):(nrow(D) - 1), ]
polydegree = 2# Almon lag 澶氶」寮忔渶楂樻椤逛负3
Ra = vector('list',2)
names(Ra) = c('fC','dfC')
Ra$fC = 1 # no tail restriction; if has set 1
Ra$dfC = 0 # no deravative restriction; if has set 1
Q = Almon_lag(polydegree = polydegree, C = Spc$Km, R = Ra)
Qrow = as.numeric(dim(Q)[1])
eg <- Qrow
NX_alm = Qrow * Nbvar
Qd.test = Almon_lag(polydegree = polydegree, C = m_day2.test, R = Ra)
Qdrow.test = as.numeric(dim(Qd.test)[1])
egd.test <- Qdrow.test
kdm.test <- as.numeric(dim(xd2.test[[1]])[2])
# QX.test = matrix(0, nrow = nrow(X.test), ncol = Nbvar * eg)
QX.test = matrix(0, nrow = nrow(X.test), ncol = Nbvar * eg + egd.test)
for (nb in 1:Nbvar) {
QX.test[ ,((nb - 1)*eg + 1):(nb*eg)] = X.test[ ,((nb - 1)*K_mon + 1):(nb*K_mon)] %*% t(Q)
}
QX.test[ ,(Nbvar*eg + 1):(Nbvar*eg + egd.test)] =  X.test[ ,(Nbvar*K_mon+1):(Nbvar*K_mon + kdm.test)] %*% t(Qd.test)
p = eg * Nbvar + egd.test
# p = eg * Nbvar
NF <- p
# ng <- Nbvar
ng <- Nbvar + 1  # number of groups. nrow(Q) is the polydegree + 1(contain 0).
# nj <- c(rep(nrow(Q), Nbvar))
nj <- c(rep(nrow(Q), Nbvar), egd.test)  # number of predictors in each group
groups <- matrix(0, nrow = ng, ncol = nrow(Q)) # groups琛ㄧずQ鐭╅樀鍙樻崲鍚庡悇涓彉閲忕殑缁村害鐨勫垎缁?. 杩欓噷涓?30缁勶紝姣忕粍瀵瑰簲2涓淮搴?
groups[1, ] <- seq_len(nj[1])
if (ng > 1){
for (i in 2:ng) {
groups[i, ] <- seq(sum(nj[1:(i - 1)]) + 1, sum(nj[1:(i - 1)]) + nj[i])
}
}
# Standalization for data. 鏁版嵁鐨勬爣鍑嗗寲鍗佸垎閲嶈
x.test = array(0, dim = c(N, NT_test, NF))
y.test = array(0, dim = c(NT_test, N))
d.test = array(0, dim = c(N, NT_test, NH))
for (n in 1:N) {
# result_x <- standardize_me2(QX.test)
result_x <- standardize_me(QX.test)
result_y <- center_me(as.matrix(Y.test))
x_scale = result_x$y
mu_x = result_x$mu
sig_x = result_x$sig
y_scale = result_y$y
mu_y = result_y$mu
x.test[n, , ] <- x_scale
y.test[ ,n] <- y_scale
d.test[n, , ] <- D_test
}
############################## For Total Forecast ################################
#### Set Forecast horizon h = 0
trial_count = 10000
Y_test_fore_total = array(0, dim = c(trial_count, N, NT_test))
state_forecast_result = array(0, dim = c(trial_count, NT_test))
for (tr in 1:trial_count){
# p0_est <- p0_gen(tau.estimation)
state_test_est <- array(0, dim = c(NT_test))
p0_est <- temp.mat[nrow(temp.mat), ] / sum(temp.mat[nrow(temp.mat), ])
initial_state <- state.estimation[length(state.estimation)]
# tran_test_est <- tran_acc(zeta.estimation, phi.estimation, d.test, NS)
# tran_p_test_est <- tran_p(tran_test_est, NS)
p_test_est <- apply(p_result, c(2, 3), mean)
state_test_prob <- state_forecast_hmm(p0_est, p_test_est, N, NT_test)
for (ts in 1:NT_test) {
state_test_est[ts] <- sample(c(1:NS), size = 1, replace = FALSE, prob = state_test_prob[ts, ])
}
state_forecast_result[tr, ] <- state_test_est
location1_test <- which(state_test_est == 1, arr.ind = TRUE)
location2_test <- which(state_test_est == 2, arr.ind = TRUE)
mean_test_fore <- array(dim = c(N, NT_test))
Y_test_fore <- array(dim = c(N, NT_test))
if(length(location1_test) > 0){
for (i in 1:length(location1_test)) {
idx <- location1_test[i,]
mean_test_fore[1, idx] <- t(x.test[1, idx, ]) %*% beta.estimation[1, ] + intercept.estimation[1]
Y_test_fore[1, idx] <- rnorm(1, mean = mean_test_fore[1, idx], sd = sqrt(sigma.estimation[1]))
}
}
if(length(location2_test) > 0){
for (i in 1:length(location2_test)) {
idx <- location2_test[i,]
mean_test_fore[1, idx] <- t(x.test[1, idx, ]) %*% beta.estimation[2, ] + intercept.estimation[2]
Y_test_fore[1, idx] <- rnorm(1, mean = mean_test_fore[1, idx], sd = sqrt(sigma.estimation[2]))
}
}
# if(nrow(location3_test) > 0){
#   for (i in 1:nrow(location3_test)) {
#     idx <- location3_test[i,]
#     mean_test_fore[1, idx] <- t(x.test[1, idx, ]) %*% beta.estimation[3, ] + intercept.estimation[3]
#     Y_test_fore[1, idx] <- rnorm(1, mean = mean_test_fore[1, idx], sd = sqrt(sigma.estimation[3]))
#   }
# }
# Y_test_fore_total[tr, , ] = abs(Y_test_fore)
Y_test_fore_total[tr, , ] = Y_test_fore
}
state.forecast.res = array(0, dim=c(N, NT_test))
temp.forecast.mat = matrix(0, nrow = NT_test, ncol = NS)
for(j in 1:NT_test){
temp = rep(0,NS)
for(k in 1:NS){
temp[k] = sum(state_forecast_result[ ,j] == k)
}
temp.forecast.mat[j, ] = temp
state.forecast.res[1, j] = which.max(temp)
}
# state.forecast.res[1, c(13:31)] <- 2
Y_test_fore_res = apply(Y_test_fore_total, c(2, 3), mean)
Y_test_fore_vec = sample_merge_Ystate(Y_test_fore_res)
Y_test_true_vec = sample_merge_Ystate(Y.test)
y_rmse = rmse(Y_test_true_vec, Y_test_fore_vec)
y_rsquare = r_squared(Y_test_true_vec, Y_test_fore_vec)
y_mae = mae(Y_test_true_vec, Y_test_fore_vec)
y_crps = crps2(Y_test_true_vec, Y_test_fore_vec)
###################### Plot for forecast ########################
m_mon <- 3
NBER = read.csv('RealData/USREC.csv', header = TRUE)
NBER_noindex <- NBER['USREC']
dates_nber_all <- seq(as.Date("1854-12-01"), by = "month", length.out = dim(NBER)[1])
NBER_xts <- xts(NBER_noindex, order.by = dates_nber_all)
dates_nber.quarter <- seq(as.Date("2011-01-01"), by = "quarter", length.out = NT_test)
dates_nber.month <- seq(as.Date("2011-01-01"), by = "month", length.out = NT_test * m_mon)
NBER_used <- NBER_xts['2011-01-01/2023-09-01']
df.nber <- data.frame(
Date = dates_nber.month,
Recession = c(as.numeric(NBER_used))
)
recession_periods <- subset(df.nber, NBER == 1)
df.nber$Date <- as.Date(df.nber$Date)
# 鍒涘缓鍖呭惈琛伴€€鏈熺殑寮€濮嬪拰缁撴潫鏃堕棿鐨勬暟鎹
recession_periods <- df.nber %>%
mutate(Recession_Change = Recession != lag(Recession, default = 0)) %>%
mutate(Period_ID = cumsum(Recession_Change)) %>%
group_by(Period_ID) %>%
filter(Recession == 1) %>%
summarize(Start = min(Date), End = max(Date))
# 鍖哄垎棰滆壊
df.gdp.growth.color <- data.frame(
Date = dates_nber.quarter,
GDP.growth = c(Y.test),
# state = as.character(real_state)
state = as.character(c(state.forecast.res))
)
plot.gdp.growth.test.color <- ggplot() +
geom_rect(data = recession_periods, aes(xmin = Start, xmax = End, ymin = -9, ymax = 9),
fill = "grey", alpha = 0.5) +
geom_point(data = df.gdp.growth.color, aes(x = Date, y = GDP.growth, color = state)) +
scale_color_manual(values = c("1" = "red", "2" = "blue")) +
labs(y = TeX("$y_t"), x = "Date") +
theme_minimal() +
theme(
axis.title.x = element_text(size = 16),  # x 杞村悕绉板瓧浣撳ぇ灏?
axis.title.y = element_text(size = 16),  # y 杞村悕绉板瓧浣撳ぇ灏?
axis.text = element_text(size = 12),     # 鍧愭爣杞村埢搴﹀瓧浣撳ぇ灏?
plot.title = element_text(size = 18, face = "bold"),  # 鏍囬瀛椾綋澶у皬
legend.text = element_text(size = 14),   # 鍥句緥瀛椾綋澶у皬
legend.title = element_text(size = 16)   # 鍥句緥鏍囬瀛椾綋澶у皬
) +
scale_x_date(date_breaks = "2 year", date_labels = "%Y") +  # 姣忓勾鏄剧ず涓€涓棩鏈熸爣绛?
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # 鏃嬭浆x杞寸殑鏂囨湰
theme(axis.text.y = element_text())  # 淇濇寔y杞寸殑鏂囨湰
show(plot.gdp.growth.test.color)
source("F:/24561/Documents/ResearchWrite/NHMM-MIDAS/Submission Requirements/Code/SomeResults/RealAnalysis/OOS_HMSP.R")
